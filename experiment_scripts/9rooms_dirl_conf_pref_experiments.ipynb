{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 500\n",
    "env_num = 2\n",
    "spec_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Learning Policy for Spec #7 in Env #2 ****\n",
      "\n",
      "**** Abstract Graph ****\n",
      "0 -> 1 2\n",
      "1 -> 3\n",
      "2 -> 3\n",
      "3 -> 3\n",
      "\n",
      "Learning policy for edge 0 -> 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shabadi/dirl/venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps taken at iteration 0: 1260\n",
      "Time taken at iteration 0: 0.009764838218688964 mins\n",
      "Expected reward at iteration 0: -358.2134984186313\n",
      "\n",
      "Steps taken at iteration 1: 2520\n",
      "Time taken at iteration 1: 0.019490158557891844 mins\n",
      "Expected reward at iteration 1: -346.93042711132637\n",
      "\n",
      "Steps taken at iteration 2: 3780\n",
      "Time taken at iteration 2: 0.029172372817993165 mins\n",
      "Expected reward at iteration 2: -339.1622479223143\n",
      "\n",
      "Steps taken at iteration 3: 5040\n",
      "Time taken at iteration 3: 0.03887265125910441 mins\n",
      "Expected reward at iteration 3: -337.21610281404423\n",
      "\n",
      "Steps taken at iteration 4: 6300\n",
      "Time taken at iteration 4: 0.048585915565490724 mins\n",
      "Expected reward at iteration 4: -332.4443357651369\n",
      "\n",
      "Steps taken at iteration 5: 7560\n",
      "Time taken at iteration 5: 0.05837599436442057 mins\n",
      "Expected reward at iteration 5: -325.552448724378\n",
      "\n",
      "Steps taken at iteration 6: 8820\n",
      "Time taken at iteration 6: 0.06818817456563314 mins\n",
      "Expected reward at iteration 6: -310.2438867124019\n",
      "\n",
      "Steps taken at iteration 7: 10080\n",
      "Time taken at iteration 7: 0.0781146248181661 mins\n",
      "Expected reward at iteration 7: -298.5872627040936\n",
      "\n",
      "Steps taken at iteration 8: 11340\n",
      "Time taken at iteration 8: 0.08797347545623779 mins\n",
      "Expected reward at iteration 8: -286.9696718389773\n",
      "\n",
      "Steps taken at iteration 9: 12600\n",
      "Time taken at iteration 9: 0.09780869483947754 mins\n",
      "Expected reward at iteration 9: -276.891519176825\n",
      "\n",
      "Steps taken at iteration 10: 13860\n",
      "Time taken at iteration 10: 0.10760182539621989 mins\n",
      "Expected reward at iteration 10: -268.3556102855911\n",
      "\n",
      "Steps taken at iteration 11: 15120\n",
      "Time taken at iteration 11: 0.11747691631317139 mins\n",
      "Expected reward at iteration 11: -258.0156544953803\n",
      "\n",
      "Steps taken at iteration 12: 16380\n",
      "Time taken at iteration 12: 0.12730374336242675 mins\n",
      "Expected reward at iteration 12: -251.60490738054418\n",
      "\n",
      "Steps taken at iteration 13: 17640\n",
      "Time taken at iteration 13: 0.13715043465296428 mins\n",
      "Expected reward at iteration 13: -253.44608308608755\n",
      "\n",
      "Steps taken at iteration 14: 18900\n",
      "Time taken at iteration 14: 0.14695430596669515 mins\n",
      "Expected reward at iteration 14: -254.32036068106768\n",
      "\n",
      "Steps taken at iteration 15: 20160\n",
      "Time taken at iteration 15: 0.1568335175514221 mins\n",
      "Expected reward at iteration 15: -243.5614336440288\n",
      "\n",
      "Steps taken at iteration 16: 21420\n",
      "Time taken at iteration 16: 0.1666843056678772 mins\n",
      "Expected reward at iteration 16: -238.31082879301252\n",
      "\n",
      "Steps taken at iteration 17: 22680\n",
      "Time taken at iteration 17: 0.17650463581085205 mins\n",
      "Expected reward at iteration 17: -235.77831034631618\n",
      "\n",
      "Steps taken at iteration 18: 23940\n",
      "Time taken at iteration 18: 0.18630408843358356 mins\n",
      "Expected reward at iteration 18: -236.1196134802101\n",
      "\n",
      "Steps taken at iteration 19: 25200\n",
      "Time taken at iteration 19: 0.1961771567662557 mins\n",
      "Expected reward at iteration 19: -235.4941238739784\n",
      "\n",
      "Steps taken at iteration 20: 26460\n",
      "Time taken at iteration 20: 0.205987807114919 mins\n",
      "Expected reward at iteration 20: -226.5719792311779\n",
      "\n",
      "Steps taken at iteration 21: 27720\n",
      "Time taken at iteration 21: 0.21583957274754842 mins\n",
      "Expected reward at iteration 21: -226.16694833381305\n",
      "\n",
      "Steps taken at iteration 22: 28980\n",
      "Time taken at iteration 22: 0.22567446231842042 mins\n",
      "Expected reward at iteration 22: -225.03205243634824\n",
      "\n",
      "Steps taken at iteration 23: 30240\n",
      "Time taken at iteration 23: 0.23560308615366618 mins\n",
      "Expected reward at iteration 23: -225.72257353121532\n",
      "\n",
      "Steps taken at iteration 24: 31500\n",
      "Time taken at iteration 24: 0.24544873237609863 mins\n",
      "Expected reward at iteration 24: -224.02755922566658\n",
      "\n",
      "Steps taken at iteration 25: 32760\n",
      "Time taken at iteration 25: 0.25532256364822387 mins\n",
      "Expected reward at iteration 25: -225.5001577826158\n",
      "\n",
      "Steps taken at iteration 26: 34020\n",
      "Time taken at iteration 26: 0.2651587684949239 mins\n",
      "Expected reward at iteration 26: -224.3898384583018\n",
      "\n",
      "Steps taken at iteration 27: 35280\n",
      "Time taken at iteration 27: 0.27506646315256755 mins\n",
      "Expected reward at iteration 27: -222.83799696686575\n",
      "\n",
      "Steps taken at iteration 28: 36540\n",
      "Time taken at iteration 28: 0.28490843772888186 mins\n",
      "Expected reward at iteration 28: -216.0336693574103\n",
      "\n",
      "Steps taken at iteration 29: 37800\n",
      "Time taken at iteration 29: 0.29471441904703777 mins\n",
      "Expected reward at iteration 29: -225.5727972932801\n",
      "\n",
      "Steps taken at iteration 30: 39060\n",
      "Time taken at iteration 30: 0.3045484264691671 mins\n",
      "Expected reward at iteration 30: -214.60345666160347\n",
      "\n",
      "Steps taken at iteration 31: 40320\n",
      "Time taken at iteration 31: 0.3143915931383769 mins\n",
      "Expected reward at iteration 31: -209.86627498212442\n",
      "\n",
      "Steps taken at iteration 32: 41580\n",
      "Time taken at iteration 32: 0.32426358461380006 mins\n",
      "Expected reward at iteration 32: -208.00817540016175\n",
      "\n",
      "Steps taken at iteration 33: 42840\n",
      "Time taken at iteration 33: 0.33416491746902466 mins\n",
      "Expected reward at iteration 33: -208.36027155446342\n",
      "\n",
      "Steps taken at iteration 34: 44100\n",
      "Time taken at iteration 34: 0.34405308961868286 mins\n",
      "Expected reward at iteration 34: -211.62546941481676\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m abstract_reach\u001b[38;5;241m.\u001b[39mpretty_print()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Step 5: Learn policy\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m path_policies \u001b[38;5;241m=\u001b[39m \u001b[43mabstract_reach\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_all_paths\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mres_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m adj_list \u001b[38;5;241m=\u001b[39m adj_list_from_task_graph(abstract_reach\u001b[38;5;241m.\u001b[39mabstract_graph)\n\u001b[1;32m    107\u001b[0m terminal_vertices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(adj_list)) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m adj_list[i]]\n",
      "File \u001b[0;32m~/dirl/spectrl/hierarchy/reachability.py:263\u001b[0m, in \u001b[0;36mAbstractReachability.learn_all_paths\u001b[0;34m(self, env, hyperparams, algo, res_model, max_steps, safety_penalty, neg_inf, alpha, num_samples, use_gpu, render, succ_thresh)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    261\u001b[0m start_dist \u001b[38;5;241m=\u001b[39m pp\u001b[38;5;241m.\u001b[39mstart_dist\n\u001b[0;32m--> 263\u001b[0m edge_policy, reach_env, log_info \u001b[38;5;241m=\u001b[39m \u001b[43medge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m final_states \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n",
      "File \u001b[0;32m~/dirl/spectrl/hierarchy/reachability.py:61\u001b[0m, in \u001b[0;36mAbstractEdge.learn_policy\u001b[0;34m(self, env, hyperparams, source_vertex, init_dist, algo, res_model, max_steps, safety_penalty, neg_inf, alpha, use_gpu, render)\u001b[0m\n\u001b[1;32m     59\u001b[0m     nn_params \u001b[38;5;241m=\u001b[39m NNParams(reach_env, hyperparams\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[1;32m     60\u001b[0m     policy \u001b[38;5;241m=\u001b[39m NNPolicy(nn_params)\n\u001b[0;32m---> 61\u001b[0m     log_info \u001b[38;5;241m=\u001b[39m \u001b[43mars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreach_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mars_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddpg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     63\u001b[0m     agent \u001b[38;5;241m=\u001b[39m DDPG(hyperparams, use_gpu\u001b[38;5;241m=\u001b[39muse_gpu)\n",
      "File \u001b[0;32m~/dirl/spectrl/rl/ars.py:159\u001b[0m, in \u001b[0;36mars\u001b[0;34m(env, nn_policy, params, cum_reward)\u001b[0m\n\u001b[1;32m    156\u001b[0m nn_policy_minus \u001b[38;5;241m=\u001b[39m _get_delta_policy(nn_policy, delta, \u001b[38;5;241m-\u001b[39mparams\u001b[38;5;241m.\u001b[39mdelta_std)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# iii) Get rollouts\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m sarss_plus \u001b[38;5;241m=\u001b[39m \u001b[43mget_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_policy_plus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m sarss_minus \u001b[38;5;241m=\u001b[39m get_rollout(env, nn_policy_minus, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    161\u001b[0m num_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(sarss_plus) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(sarss_minus))\n",
      "File \u001b[0;32m~/dirl/spectrl/util/rl.py:101\u001b[0m, in \u001b[0;36mget_rollout\u001b[0;34m(env, policy, render, max_timesteps, stateful_policy, init_state)\u001b[0m\n\u001b[1;32m     98\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Step 2b: Action\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Step 2c: Transition environment\u001b[39;00m\n\u001b[1;32m    104\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/dirl/spectrl/rl/ars.py:94\u001b[0m, in \u001b[0;36mNNPolicy.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     91\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(state))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Step 4: Apply the hidden layer\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Step 5: Apply the output layer\u001b[39;00m\n\u001b[1;32m     97\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(hidden))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from conformal.all_paths_conformal_pred import all_paths_conformal_pred\n",
    "from conformal.bucketed_conformal_pred import bucketed_conformal_pred\n",
    "from conformal.nonconformity_score_graph import DIRLCumRewardScoreGraph, DIRLTimeTakenScoreGraph\n",
    "from spectrl.hierarchy.construction import adj_list_from_task_graph, automaton_graph_from_spec\n",
    "from spectrl.hierarchy.reachability import HierarchicalPolicy, ConstrainedEnv\n",
    "from spectrl.main.spec_compiler import ev, seq, choose, alw\n",
    "from spectrl.util.io import parse_command_line_options, save_log_info, save_object\n",
    "from spectrl.util.rl import print_performance, get_rollout\n",
    "from spectrl.rl.ars import HyperParams\n",
    "\n",
    "from spectrl.examples.rooms_envs import (\n",
    "    GRID_PARAMS_LIST,\n",
    "    MAX_TIMESTEPS,\n",
    "    START_ROOM,\n",
    "    FINAL_ROOM,\n",
    ")\n",
    "from spectrl.envs.rooms import RoomsEnv\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "render = False\n",
    "folder = ''\n",
    "itno = -1\n",
    "\n",
    "log_info = []\n",
    "\n",
    "grid_params = GRID_PARAMS_LIST[env_num]\n",
    "\n",
    "hyperparams = HyperParams(30, num_iters, 30, 15, 0.05, 0.3, 0.15)\n",
    "\n",
    "print(\n",
    "    \"\\n**** Learning Policy for Spec #{} in Env #{} ****\".format(\n",
    "        spec_num, env_num\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 1: initialize system environment\n",
    "system = RoomsEnv(grid_params, START_ROOM[env_num], FINAL_ROOM[env_num])\n",
    "\n",
    "# Step 4: List of specs.\n",
    "if env_num == 2:\n",
    "    bottomright = (0, 2)\n",
    "    topleft = (2, 0)\n",
    "if env_num == 3 or env_num == 4:\n",
    "    bottomright = (0, 3)\n",
    "    topleft = (3, 0)\n",
    "\n",
    "# test specs\n",
    "spec0 = ev(grid_params.in_room(FINAL_ROOM[env_num]))\n",
    "spec1 = seq(\n",
    "    ev(grid_params.in_room(FINAL_ROOM[env_num])),\n",
    "    ev(grid_params.in_room(START_ROOM[env_num])),\n",
    ")\n",
    "spec2 = ev(grid_params.in_room(topleft))\n",
    "\n",
    "# Goto destination, return to initial\n",
    "spec3 = seq(\n",
    "    ev(grid_params.in_room(topleft)),\n",
    "    ev(grid_params.in_room(START_ROOM[env_num])),\n",
    ")\n",
    "# Choose between top-right and bottom-left blocks (Same difficulty - learns 3/4 edges)\n",
    "spec4 = choose(\n",
    "    ev(grid_params.in_room(bottomright)), ev(grid_params.in_room(topleft))\n",
    ")\n",
    "# Choose between top-right and bottom-left, then go to Final state (top-right).\n",
    "# Only one path is possible (learns 5/5 edges. Should have a bad edge)\n",
    "spec5 = seq(\n",
    "    choose(\n",
    "        ev(grid_params.in_room(bottomright)), ev(grid_params.in_room(topleft))\n",
    "    ),\n",
    "    ev(grid_params.in_room(FINAL_ROOM[env_num])),\n",
    ")\n",
    "# Add obsacle towards topleft\n",
    "spec6 = alw(grid_params.avoid_center((1, 0)), ev(grid_params.in_room(topleft)))\n",
    "# Either go to top-left or bottom-right. obstacle on the way to top-left.\n",
    "# Then, go to Final state. Only one route is possible\n",
    "spec7 = seq(\n",
    "    choose(\n",
    "        alw(grid_params.avoid_center((1, 0)), ev(grid_params.in_room(topleft))),\n",
    "        ev(grid_params.in_room(bottomright)),\n",
    "    ),\n",
    "    ev(grid_params.in_room(FINAL_ROOM[env_num])),\n",
    ")\n",
    "\n",
    "specs = [spec0, spec1, spec2, spec3, spec4, spec5, spec6, spec7]\n",
    "\n",
    "# Step 3: construct abstract reachability graph\n",
    "_, abstract_reach = automaton_graph_from_spec(specs[spec_num])\n",
    "print(\"\\n**** Abstract Graph ****\")\n",
    "abstract_reach.pretty_print()\n",
    "\n",
    "# Step 5: Learn policy\n",
    "path_policies = abstract_reach.learn_all_paths(\n",
    "    system,\n",
    "    hyperparams,\n",
    "    res_model=None,\n",
    "    max_steps=20,\n",
    "    render=render,\n",
    "    neg_inf=-100,\n",
    "    safety_penalty=-1,\n",
    "    num_samples=500,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "adj_list = adj_list_from_task_graph(abstract_reach.abstract_graph)\n",
    "terminal_vertices = [i for i in range(len(adj_list)) if i in adj_list[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketed:\n",
      "[0, 1, 3]\n",
      "[82, 18]\n",
      "[21, 20]\n",
      "21\n",
      "\n",
      "All paths:\n",
      "(0, 1, 3)\n",
      "[21, 19]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#### e = 0.1 ; time taken scores\n",
    "\n",
    "time_taken_score_graph = DIRLTimeTakenScoreGraph(adj_list, path_policies)\n",
    "e = 0.1\n",
    "n_samples = 500\n",
    "total_buckets = 100\n",
    "vbs = bucketed_conformal_pred(time_taken_score_graph, e, total_buckets, n_samples)\n",
    "min_path, min_path_scores = all_paths_conformal_pred(time_taken_score_graph, e, n_samples)\n",
    "\n",
    "vb = vbs.buckets[(terminal_vertices[0], total_buckets)]\n",
    "print(\"Bucketed:\")\n",
    "print(vb.path)\n",
    "print(vb.path_buckets)\n",
    "print(vb.path_score_quantiles)\n",
    "print(max(vb.path_score_quantiles))\n",
    "\n",
    "print()\n",
    "print(\"All paths:\")\n",
    "print(min_path)\n",
    "print(min_path_scores)\n",
    "print(max(min_path_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketed:\n",
      "[0, 1, 3]\n",
      "[172, 28]\n",
      "[21, 20]\n",
      "21\n",
      "\n",
      "All paths:\n",
      "(0, 1, 3)\n",
      "[21, 19]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#### e = 0.1 ; time taken scores\n",
    "\n",
    "time_taken_score_graph = DIRLTimeTakenScoreGraph(adj_list, path_policies)\n",
    "e = 0.1\n",
    "n_samples = 500\n",
    "total_buckets = 200\n",
    "vbs = bucketed_conformal_pred(time_taken_score_graph, e, total_buckets, n_samples)\n",
    "min_path, min_path_scores = all_paths_conformal_pred(time_taken_score_graph, e, n_samples)\n",
    "\n",
    "vb = vbs.buckets[(terminal_vertices[0], total_buckets)]\n",
    "print(\"Bucketed:\")\n",
    "print(vb.path)\n",
    "print(vb.path_buckets)\n",
    "print(vb.path_score_quantiles)\n",
    "print(max(vb.path_score_quantiles))\n",
    "\n",
    "print()\n",
    "print(\"All paths:\")\n",
    "print(min_path)\n",
    "print(min_path_scores)\n",
    "print(max(min_path_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketed:\n",
      "[0, 1, 3]\n",
      "[1, 1]\n",
      "[21, 20]\n",
      "21\n",
      "\n",
      "All paths:\n",
      "(0, 1, 3)\n",
      "[21, 20]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#### e = 0.1 ; time taken scores\n",
    "\n",
    "time_taken_score_graph = DIRLTimeTakenScoreGraph(adj_list, path_policies)\n",
    "e = 0.05\n",
    "n_samples = 500\n",
    "total_buckets = 2\n",
    "vbs = bucketed_conformal_pred(time_taken_score_graph, e, total_buckets, n_samples)\n",
    "min_path, min_path_scores = all_paths_conformal_pred(time_taken_score_graph, e, n_samples)\n",
    "\n",
    "vb = vbs.buckets[(terminal_vertices[0], total_buckets)]\n",
    "print(\"Bucketed:\")\n",
    "print(vb.path)\n",
    "print(vb.path_buckets)\n",
    "print(vb.path_score_quantiles)\n",
    "print(max(vb.path_score_quantiles))\n",
    "\n",
    "print()\n",
    "print(\"All paths:\")\n",
    "print(min_path)\n",
    "print(min_path_scores)\n",
    "print(max(min_path_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n"
     ]
    }
   ],
   "source": [
    "time_taken_score_graph = DIRLTimeTakenScoreGraph(adj_list, path_policies)\n",
    "n_samples = 500\n",
    "es = [0.2, 0.1, 0.05]\n",
    "total_buckets = [5, 10, 20, 25]\n",
    "\n",
    "data_time_taken = dict()\n",
    "data_time_taken[\"metadata\"] = {\"es\": es, \"total_buckets\": total_buckets, \"scores\": \"time-taken\", \"env\": \"9-rooms\", \"spec\": spec_num, \"n_samples\": n_samples}\n",
    "\n",
    "for e in es:\n",
    "    e_data = dict()\n",
    "    for buckets in total_buckets:\n",
    "        bucket_data = dict()\n",
    "        vbs = bucketed_conformal_pred(time_taken_score_graph, e, buckets, n_samples)\n",
    "        min_path, min_path_scores = all_paths_conformal_pred(time_taken_score_graph, e, n_samples)\n",
    "        vb = vbs.buckets[(terminal_vertices[0], buckets)]\n",
    "\n",
    "        bucket_data[\"bucketed\"] = {\"path\": vb.path, \n",
    "                                   \"path_buckets\": vb.path_buckets, \n",
    "                                   \"path_score_quantiles\": vb.path_score_quantiles, \n",
    "                                   \"max_path_score_quantile\": max(vb.path_score_quantiles)}\n",
    "        bucket_data[\"all-paths\"] = {\"path\": min_path, \"min_path_scores\": min_path_scores, \"max_min_path_scores\": max(min_path_scores)}\n",
    "        e_data[buckets] = bucket_data\n",
    "    data_time_taken[str(e)] = e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert the Python object to a JSON string\n",
    "json_data = json.dumps(data_time_taken, indent=2)\n",
    "\n",
    "# Store the JSON string in a file\n",
    "with open(\"conformal_experiments_data/9rooms-spec7-time-taken.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n",
      "changed again\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import conformal.all_paths_conformal_pred as ap\n",
    "import conformal.bucketed_conformal_pred as bcp\n",
    "reload(bcp)\n",
    "import conformal.bucketed_conformal_pred as bcp\n",
    "from conformal.all_paths_conformal_pred import all_paths_conformal_pred\n",
    "from conformal.bucketed_conformal_pred import bucketed_conformal_pred\n",
    "\n",
    "time_taken_score_graph = DIRLCumRewardScoreGraph(adj_list, path_policies)\n",
    "n_samples = 500\n",
    "es = [0.2, 0.1, 0.05]\n",
    "total_buckets = [5, 10, 20, 25, 50]\n",
    "\n",
    "data_time_taken = dict()\n",
    "data_time_taken[\"metadata\"] = {\"es\": es, \"total_buckets\": total_buckets, \"scores\": \"time-taken\", \"env\": \"9-rooms\", \"spec\": spec_num, \"n_samples\": n_samples}\n",
    "\n",
    "for e in es:\n",
    "    e_data = dict()\n",
    "    for buckets in total_buckets:\n",
    "        bucket_data = dict()\n",
    "        vbs = bcp.bucketed_conformal_pred(time_taken_score_graph, e, buckets, n_samples)\n",
    "        min_path, min_path_scores = ap.all_paths_conformal_pred(time_taken_score_graph, e, n_samples)\n",
    "        vb = vbs.buckets[(terminal_vertices[0], buckets)]\n",
    "\n",
    "        bucket_data[\"bucketed\"] = {\"path\": vb.path, \n",
    "                                   \"path_buckets\": vb.path_buckets, \n",
    "                                   \"path_score_quantiles\": vb.path_score_quantiles, \n",
    "                                   \"max_path_score_quantile\": max(vb.path_score_quantiles)}\n",
    "        bucket_data[\"all-paths\"] = {\"path\": min_path, \"min_path_scores\": min_path_scores, \"max_min_path_scores\": max(min_path_scores)}\n",
    "        e_data[buckets] = bucket_data\n",
    "    data_time_taken[str(e)] = e_data\n",
    "\n",
    "# Convert the Python object to a JSON string\n",
    "json_data = json.dumps(data_time_taken, indent=2)\n",
    "\n",
    "# Store the JSON string in a file\n",
    "with open(\"conformal_experiments_data/9rooms-spec7-cum-reward.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(-0.002153621251416382), np.float64(-0.0075586513796928045)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vbs.buckets[(3,15)].path_score_quantiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
